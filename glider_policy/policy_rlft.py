import os
import json
import argparse
from datetime import datetime

import numpy as np
import matplotlib
matplotlib.use('Agg')

import torch
from torch import nn

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

from rl_env import GliderRNNEnv
from policy_trainer import Policy 


class RNNFeatureExtractor(BaseFeaturesExtractor):
    """
    ä½¿ç”¨ä½  supervised è®­ç»ƒå¥½çš„ GRU ä½œä¸º PPO çš„ç‰¹å¾æå–å™¨ï¼š
    - è¾“å…¥: obs (B, n_hist, 9)
    - è¾“å‡º: (B, hidden_dim)
    - å†…éƒ¨åŒ…å«: æ ‡å‡†åŒ– + GRU
    """

    def __init__(self, observation_space, n_hist, ckpt_path, policy_type="gru"):
        # å…ˆåŠ è½½ checkpointï¼Œæ‹¿åˆ° hidden_dim
        ckpt = torch.load(ckpt_path, map_location="cpu")
        hidden_dim = int(ckpt["hidden_dim"])
        n_hist_ckpt = int(ckpt["n_hist"])

        if n_hist_ckpt != n_hist:
            print(f"[WARN] ckpt ä¸­çš„ n_hist={n_hist_ckpt} ä¸å½“å‰ env çš„ n_hist={n_hist} ä¸ä¸€è‡´ï¼Œ"
                  f"è¯·ç¡®è®¤æ˜¯å¦æœ‰é—®é¢˜ã€‚è¿™é‡Œä»ç„¶å¼ºè¡Œä½¿ç”¨ n_hist={n_hist} çš„ obsã€‚")

        super().__init__(observation_space, features_dim=hidden_dim)

        # è®°å½• n_hist & è¾“å…¥ç»´åº¦
        self.n_hist = n_hist
        self.input_dim = 9
        self.policy_type = ckpt.get("policy_type", policy_type)

        # ====== 1) è¾“å…¥æ ‡å‡†åŒ–å‚æ•° ======
        input_mean = ckpt["input_mean"].astype(np.float32)  # (1, 9)
        input_std = ckpt["input_std"].astype(np.float32)

        # ç”¨ buffer å­˜ï¼Œè¿™æ ·ä¼šè‡ªåŠ¨è·Ÿç€æ¨¡å‹åˆ° GPU / CPU
        self.register_buffer("input_mean", torch.from_numpy(input_mean))  # (1, 9)
        self.register_buffer("input_std", torch.from_numpy(input_std))

        # ====== 2) æ„å»ºä¸ supervised å®Œå…¨ç›¸åŒçš„ Policyï¼Œç„¶åæ‹·è´å‚æ•° ======
        self.core_policy = Policy(
            input_dim=self.input_dim,
            hidden_dim=hidden_dim,
            num_layers=2,
            act_dim=3,
            policy_type=self.policy_type,
        )

        # åŸå§‹ ckpt é‡Œçš„æ•´ä¸ª state_dict
        pretrained_state = ckpt["model_state"]
        current_state = self.core_policy.state_dict()

        # åªè¦†ç›–åŒ¹é…çš„é”®ï¼ˆåŒ…æ‹¬ rnn å’Œ fcï¼Œå…¶å®éƒ½å¯ä»¥ï¼Œæå–å™¨åªç”¨ rnnï¼‰
        for k in current_state.keys():
            if k in pretrained_state:
                current_state[k] = pretrained_state[k]
        self.core_policy.load_state_dict(current_state)

        # å–å‡º RNN éƒ¨åˆ†ï¼Œä¹‹å forward åªç”¨å®ƒ
        self.rnn = self.core_policy.rnn

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        observations: (B, n_hist, 9)
        è¿”å›: (B, hidden_dim)
        """
        # SB3 ç»™çš„ obs æ˜¯ (B, n_hist, 9)ï¼Œç¡®ä¿ä¸€ä¸‹ dtype
        x = observations.float()

        # broadcast (1,9) åˆ° (1,1,9) å†åˆ° (B, n_hist, 9)
        mean = self.input_mean.view(1, 1, -1)
        std = self.input_std.view(1, 1, -1)

        x_norm = (x - mean) / std

        # ç›´æ¥ç”¨ GRU
        out, h = self.rnn(x_norm)   # out: (B, n_hist, hidden_dim)
        last = out[:, -1, :]        # (B, hidden_dim)

        return last


class RNNTrainingCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.reset_stats()

    def reset_stats(self):
        self.episode_rewards = []
        self.episode_distances = []
        self.temp_rewards = []
        self.temp_distances = []

    def _on_step(self):
        if self.locals is not None and 'infos' in self.locals and len(self.locals['infos']) > 0:
            info = self.locals['infos'][0]
            if "reward" in info:
                self.temp_rewards.append(info["reward"])
                self.logger.record("train/reward", info["reward"])
            if "distance_to_goal_m" in info:
                self.temp_distances.append(info["distance_to_goal_m"])
                self.logger.record("train/distance_to_goal_m", info["distance_to_goal_m"])
        return True

    def on_rollout_end(self):
        if len(self.temp_rewards) > 0:
            mean_r = float(np.mean(self.temp_rewards))
            mean_d = float(np.mean(self.temp_distances)) if len(self.temp_distances) > 0 else np.nan

            self.episode_rewards.append(mean_r)
            self.episode_distances.append(mean_d)

            self.logger.record("rollout/avg_reward", mean_r)
            self.logger.record("rollout/avg_distance_to_goal_m", mean_d)

        self.temp_rewards = []
        self.temp_distances = []

        self.logger.dump(self.num_timesteps)
        return True


callback = RNNTrainingCallback()
run_id = datetime.now().strftime("%Y%m%d-%H%M%S")


try:
    with open("optimization_results/best_parameters.json", "r") as f:
        params = json.load(f)
    print("Successfully loaded best training parameters:", params)
except FileNotFoundError:
    print("WARNING: Best parameters file not found, using default training parameters")
    params = {
        "learning_rate": 3e-4,
        "n_steps": 2048,
        "batch_size": 64,
        "n_epochs": 10,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "clip_range": 0.2,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
    }


CSV_DIR = "/home/star/helong/repos/drones/drones-MetaActions-new/data/raw_data1025/"


def find_csv_files(data_dir):
    files = sorted([f for f in os.listdir(data_dir) if f.endswith(".csv")])
    csv_paths = [os.path.join(data_dir, f) for f in files]
    if len(csv_paths) == 0:
        raise RuntimeError(f"No CSV trajectory files found in {data_dir}")
    return csv_paths


def train(
    ckpt_path,                # ğŸ‘ˆ ä½  supervised çš„ .pt
    cuda_id=0,
    model_save_path=None,
    n_hist=10,
    max_steps=200,
    arrival_threshold_m=1_000.0,
    distance_scale_m=50_000.0,
    imitation_weight=0.0,
):
    if not os.path.exists(ckpt_path):
        raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")

    csv_paths = find_csv_files(CSV_DIR)
    print(f"åœ¨ {CSV_DIR} ä¸­æ‰¾åˆ° {len(csv_paths)} æ¡ CSV è½¨è¿¹æ•°æ®ã€‚")

    # å…ˆç”¨ç¬¬ä¸€æ¡è½¨è¿¹åˆå§‹åŒ– env å’Œ PPO æ¨¡å‹
    first_csv = csv_paths[0]
    print(f"åˆå§‹åŒ–ç¯å¢ƒä½¿ç”¨è½¨è¿¹: {os.path.basename(first_csv)}")

    def make_env(path):
        return GliderRNNEnv(
            csv_path=path,
            n_hist=n_hist,
            max_steps=max_steps,
            start_from_learn_segment=True,
            arrival_threshold_m=arrival_threshold_m,
            distance_scale_m=distance_scale_m,
            imitation_weight=imitation_weight,
        )

    env = DummyVecEnv([lambda: make_env(first_csv)])

    if model_save_path is None:
        model_save_path = run_id

    device = "cpu"  # å¦‚æœåé¢æƒ³ç”¨ GPUï¼Œå¯ä»¥æ”¹æˆ f"cuda:{cuda_id}"
    print(f"\n{'='*50}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(cuda_id)}")
    print(f"å†å²çª—å£ n_hist: {n_hist}")
    print(f"max_steps: {max_steps}")
    print(f"{'='*50}\n")

    obs_dim = n_hist * 9
    print(f"è§‚æµ‹ç©ºé—´ flatten åç»´åº¦ï¼ˆç”¨äºæ£€æŸ¥ï¼‰: {obs_dim}")

    # è¿™é‡Œçš„ net_arch åªä½œç”¨äº PPO é¡¶å±‚çš„ policy/value MLPï¼Œ
    # GRU éƒ¨åˆ†åœ¨ RNNFeatureExtractor é‡Œå·²ç»æ¥è‡ªä½ çš„ checkpointã€‚
    policy_kwargs = dict(
        net_arch=[dict(pi=[128, 128], vf=[128, 128])],
        features_extractor_class=RNNFeatureExtractor,
        features_extractor_kwargs=dict(
            n_hist=n_hist,
            ckpt_path=ckpt_path,
            policy_type="gru",   # æˆ–ä» ckpt ä¸­è¯»
        ),
    )

    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=f"./glider_rnn_ppo_ft_tensorboard/{run_id}",
        device=device,
        policy_kwargs=policy_kwargs,
        **params,
    )

    # ========= éå†æ‰€æœ‰è½¨è¿¹åš finetune =========
    total_traces = len(csv_paths)
    for trace_idx, csv_path in enumerate(csv_paths, start=1):
        print(f"\nå¾®è°ƒè½¨è¿¹ {trace_idx}/{total_traces}: {os.path.basename(csv_path)}")

        env = DummyVecEnv([lambda p=csv_path: make_env(p)])
        model.set_env(env)

        callback.reset_stats()

        # æ¯æ¡è½¨è¿¹ä¸Šè·‘å›ºå®š step æ•°ï¼Œå¯ä»¥æŒ‰éœ€è¦è°ƒ
        model.learn(
            total_timesteps=10_000,
            callback=callback,
            reset_num_timesteps=False,
        )

        save_dir = os.path.join("./ckpt/rnn_ppo_finetune", model_save_path)
        os.makedirs(save_dir, exist_ok=True)
        model.save(os.path.join(save_dir, f"ppo_glider_rnn_trace{trace_idx}"))

    final_path = os.path.join("./ckpt/rnn_ppo_finetune", model_save_path, "ppo_glider_rnn_final")
    model.save(final_path)
    print(f"\nRL å¾®è°ƒç»“æŸï¼Œæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ PPO å¯¹ supervised è®­ç»ƒå¥½çš„ GRU Policy è¿›è¡Œ RL å¾®è°ƒ")
    parser.add_argument("--ckpt_path", type=str, required=True, help="supervised è®­ç»ƒå¾—åˆ°çš„ .pt checkpoint è·¯å¾„")
    parser.add_argument("--cuda", type=int, default=0, help="CUDA è®¾å¤‡ç¼–å·ï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰")
    parser.add_argument("--save_path", type=str, default=None, help="æ¨¡å‹ä¿å­˜ç›®å½•å")
    parser.add_argument("--n_hist", type=int, default=10, help="å†å²çª—å£é•¿åº¦ n_histï¼ˆéœ€ä¸ ckpt å¤§è‡´ä¸€è‡´ï¼‰")
    parser.add_argument("--max_steps", type=int, default=200, help="æ¯ä¸ª episode æœ€å¤§æ­¥æ•°")
    parser.add_argument("--arrival_threshold", type=float, default=1_000.0, help="åˆ¤å®šåˆ°è¾¾ç»ˆç‚¹çš„è·ç¦»é˜ˆå€¼ (m)")
    parser.add_argument("--distance_scale", type=float, default=50_000.0, help="è·ç¦» shaping çš„å°ºåº¦ (m)")
    parser.add_argument("--imitation_weight", type=float, default=0.0, help="æ¨¡ä»¿åŸå§‹è½¨è¿¹çš„æƒé‡ï¼ˆ0 è¡¨ç¤ºä¸æ¨¡ä»¿ï¼‰")

    args = parser.parse_args()

    train(
        ckpt_path=args.ckpt_path,
        cuda_id=args.cuda,
        model_save_path=args.save_path,
        n_hist=args.n_hist,
        max_steps=args.max_steps,
        arrival_threshold_m=args.arrival_threshold,
        distance_scale_m=args.distance_scale,
        imitation_weight=args.imitation_weight,
    )
