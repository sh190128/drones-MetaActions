import argparse
import os

import numpy as np
import torch
import matplotlib.pyplot as plt
from stable_baselines3 import PPO

# è¿™é‡ŒæŒ‰ä½ çš„å·¥ç¨‹å®é™…æƒ…å†µä¿®æ”¹å¯¼å…¥è·¯å¾„
from policy_trainer import (   # ğŸ‘ˆ æ¢æˆä½ åŸæ¥é‚£ä»½RNNè„šæœ¬çš„æ–‡ä»¶åï¼ˆä¸å¸¦.pyï¼‰
    load_trajectory,
    compute_dt,
    find_learn_segment,
    simulate_step,
)


def test_ppo_policy(args):
    """
    ä½¿ç”¨ç» RL å¾®è°ƒåçš„ PPO æ¨¡å‹ï¼Œåœ¨ä¸€æ¡ç»™å®šè½¨è¿¹ä¸Šè¿›è¡Œ rolloutï¼Œ
    å¹¶ä¸åŸå§‹è½¨è¿¹åšå¯¹æ¯”ï¼ˆç»“æ„å‚è€ƒä½ åŸæ¥çš„ test_policyï¼‰ã€‚
    """

    # 1. åŠ è½½ PPO æ¨¡å‹ï¼ˆé‡Œé¢å·²ç»åŒ…å« RNN ç‰¹å¾æå–å™¨ + å¾®è°ƒåçš„ GRU æƒé‡ï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Loading PPO model from: {args.model_path}")
    model = PPO.load(args.model_path, device=device)

    n_hist = args.n_hist

    # 2. è¯»å–æµ‹è¯•è½¨è¿¹
    states_abs, h, t = load_trajectory(args.traj_csv)
    dt = compute_dt(t)
    T = len(states_abs)

    lam0, phi0, r0 = states_abs[0, 1:4]
    goal_abs = states_abs[-1, 1:4].copy()
    goal_rel = np.array(
        [goal_abs[0] - lam0, goal_abs[1] - phi0, goal_abs[2] - r0],
        dtype=np.float32,
    )

    states_rel = states_abs.copy()
    states_rel[:, 1] -= lam0
    states_rel[:, 2] -= phi0
    states_rel[:, 3] -= r0

    learn_start = find_learn_segment(states_abs[:, 0], h)
    if learn_start is None:
        print("This trajectory has no learning segment; nothing to predict.")
        return

    print(f"Learning segment starts at index {learn_start} (time={t[learn_start]}s)")

    # 3. æ„é€ é¢„æµ‹è½¨è¿¹ (ç»å¯¹/ç›¸å¯¹çŠ¶æ€)
    pred_abs = np.zeros_like(states_abs)
    pred_abs[: learn_start + 1] = states_abs[: learn_start + 1].copy()
    pred_rel = np.zeros_like(states_rel)
    pred_rel[: learn_start + 1] = states_rel[: learn_start + 1].copy()

    # 4. åˆå§‹åŒ–å†å²çª—å£ï¼ˆä»çœŸå®æ®µå¼€å§‹ï¼‰
    #    ä¸åŸ test_policy å®Œå…¨ä¸€è‡´
    start_idx = learn_start + 1 - n_hist
    hist_states = [states_rel[i].copy() for i in range(start_idx, learn_start + 1)]
    if learn_start + 1 < n_hist:
        pad_num = n_hist - (learn_start + 1)
        pad = [states_rel[0].copy()] * pad_num
        hist_states = pad + hist_states
    hist_states = hist_states[-n_hist:]

    # 5. ä» learn_start å¼€å§‹å¾ªç¯é¢„æµ‹
    for t_idx in range(learn_start, T - 1):
        current_rel = pred_rel[t_idx]
        current_abs = pred_abs[t_idx]

        # å†å²çª—å£æ›´æ–°ï¼šç”¨å½“å‰â€œé¢„æµ‹â€çš„çŠ¶æ€
        hist_states.append(current_rel.copy())
        if len(hist_states) > n_hist:
            hist_states.pop(0)

        hist_arr = np.stack(hist_states, axis=0)  # (n_hist, 6)
        goal_tile = np.repeat(goal_rel[None, :], n_hist, axis=0)
        seq_input = np.concatenate([hist_arr, goal_tile], axis=-1)  # (n_hist, 9)

        # æ³¨æ„ï¼šPPO æ¨¡å‹å†…éƒ¨çš„ RNNFeatureExtractor ä¼šè‡ªå·±åšæ ‡å‡†åŒ–ï¼Œ
        # è¿™é‡Œç›´æ¥ç»™â€œåŸå§‹â€çš„ seq_input å³å¯ï¼Œä¸è¦å†æ‰‹åŠ¨ (x-mean)/stdã€‚
        obs = seq_input.astype(np.float32)

        # stable-baselines3 çš„ predict æ¥å—ä¸€ä¸ªå•ä¸ª obsï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨åŠ  batch ç»´
        action, _ = model.predict(obs, deterministic=True)  # action: (3,)

        # ç”¨ç®€åŒ–åŠ¨åŠ›å­¦æ»šä¸€æ­¥ï¼ˆä¸ä½ åŸæ¥çš„ test_policy ä¸€è‡´ï¼‰
        next_rel, next_abs = simulate_step(current_rel, current_abs, action, dt, lam0, phi0, r0)
        pred_rel[t_idx + 1] = next_rel
        pred_abs[t_idx + 1] = next_abs

    # 6. ç”»å›¾å¯¹æ¯”ï¼šÎ» / Ï† / r
    lam_pred = pred_abs[:, 1]
    lam_true = states_abs[:, 1]
    phi_pred = pred_abs[:, 2]
    phi_true = states_abs[:, 2]
    r_pred = pred_abs[:, 3]
    r_true = states_abs[:, 3]

    fig, axs = plt.subplots(3, 1, figsize=(10, 12))
    axs[0].plot(lam_pred, color="blue", label="Predicted", linestyle="-")
    axs[0].plot(lam_true, color="red", label="Target", linestyle="--")
    axs[0].axvline(learn_start, color="black", linestyle="--")
    axs[0].set_title("Longitude")
    axs[0].legend()

    axs[1].plot(phi_pred, color="blue", label="Predicted", linestyle="-")
    axs[1].plot(phi_true, color="red", label="Target", linestyle="--")
    axs[1].axvline(learn_start, color="black", linestyle="--")
    axs[1].set_title("Latitude")
    axs[1].legend()

    axs[2].plot(r_pred, color="blue", label="Predicted", linestyle="-")
    axs[2].plot(r_true, color="red", label="Target", linestyle="--")
    axs[2].axvline(learn_start, color="black", linestyle="--")
    axs[2].set_title("Earth Center Distance")
    axs[2].legend()

    plt.tight_layout()

    os.makedirs("./results", exist_ok=True)
    save_path = "./results/finetuned_policy.png"
    fig.savefig(save_path)

    # ---- 3D è½¨è¿¹å¯¹æ¯” ----
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')
    # ä½¿ç”¨é¢„æµ‹å’ŒçœŸå®è½¨è¿¹çš„ Î»/Ï†/r ä½œä¸ºä¸‰ç»´åæ ‡
    ax.plot(lam_pred, phi_pred, r_pred, 'b-', label='Predicted')
    ax.plot(lam_true, phi_true, r_true, 'r--', label='Target')
    ax.set_title('3D Trajectory')
    ax.set_xlabel('Longitude')
    ax.set_ylabel('Latitude')
    ax.set_zlabel('Earth Center Distance')
    ax.legend()

    plt.tight_layout()
    save_path = "./results/finetuned_policy_3d.png"
    fig.savefig(save_path)

    # 7. ç»ˆç‚¹è¯¯å·®
    final_err = np.linalg.norm(pred_abs[-1, 1:4] - states_abs[-1, 1:4])
    print(f"Final endpoint error (Î»,Ï†,r Euclidean) = {final_err:.3f}")


def main():
    parser = argparse.ArgumentParser(description="Test RL-finetuned PPO policy on a single trajectory")
    parser.add_argument("--model_path", type=str, required=True, help="PPO RL å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„ï¼ˆ.zipï¼‰")
    parser.add_argument("--traj_csv", type=str, required=True, help="ç”¨æ¥æµ‹è¯•çš„è½¨è¿¹ CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--n_hist", type=int, default=10, help="å†å²çª—å£é•¿åº¦ï¼ˆéœ€è¦å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰")

    args = parser.parse_args()
    test_ppo_policy(args)


if __name__ == "__main__":
    main()
