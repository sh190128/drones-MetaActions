# 无人机轨迹规划-模仿学习

基于最大熵逆强化学习(MaxEnt IRL)的无人机轨迹模仿学习实现。

## 设计思路

针对无人机强化学习产生累计偏差的问题，本项目引入了模仿学习技术，通过观察专家轨迹来学习无人机飞行策略。主要包括两个关键部分：

1. **逆强化学习(IRL)**: 从专家轨迹中学习隐含的奖励函数
2. **策略优化**: 使用学习到的奖励函数训练无人机控制策略

### 关键特性

- **最大熵IRL算法**: 采用最大熵原则进行奖励函数推断，避免过拟合
- **特征工程**: 提取了包括速度、位置、方向等10种不同特征
- **专家轨迹提取**: 从历史数据中自动提取专家轨迹的状态转换和动作
- **可学习的奖励网络**: 使用神经网络表示奖励函数，提高表达能力
- **无缝集成**: 与现有环境和模型兼容，保持数据接口一致

## 文件说明

- **drone_env_IL.py**: 模仿学习环境，负责与专家轨迹交互，提取特征和动作
- **train_IL.py**: 训练脚本，实现MaxEnt IRL算法和策略优化

## 使用方法

### 环境准备
```bash
pip install stable-baselines3 numpy torch matplotlib
```

### 训练模型
```bash
# 基本训练，使用默认参数
python train_IL.py

# 指定训练参数
python train_IL.py --trace_id 1 --n_obs 5 --max_steps 100 --irl_iters 10 --policy_timesteps 100000
```

### 参数说明

- `--cuda`: CUDA设备编号 (默认: 0)
- `--n_obs`: 历史观测窗口大小 (默认: 5)
- `--max_steps`: 每个episode的最大步数 (默认: 100)
- `--trace_id`: 使用哪条轨迹数据(1-8) (默认: 1)
- `--gamma`: 折扣因子 (默认: 0.99)
- `--lr`: IRL学习率 (默认: 0.01)
- `--irl_iters`: IRL迭代次数 (默认: 10)
- `--irl_samples`: 每次IRL迭代的样本数 (默认: 2000)
- `--fe_samples`: 计算特征期望的样本数 (默认: 100)
- `--trajs_per_sample`: 每个样本的轨迹数 (默认: 5)
- `--traj_length`: 每条轨迹的长度 (默认: None，使用max_steps)
- `--policy_timesteps`: 策略训练总步数 (默认: 100000)

## 模仿学习与强化学习的比较

相比于传统的强化学习方法，模仿学习具有以下优势：

1. **更快的收敛速度**: 通过模仿专家行为，避免了随机探索的过程
2. **更稳定的轨迹**: 减少了累积误差，使得无人机更准确地跟踪目标轨迹
3. **更少的训练样本**: 通过利用专家知识，需要更少的训练数据
4. **更灵活的目标泛化**: 学习到的策略可以适应不同的任务和环境

## 算法原理

最大熵逆强化学习（MaxEnt IRL）的核心思想是：找到一个奖励函数，使得专家策略在该奖励函数下，比所有其他策略都有更高的期望回报，同时策略熵最大。

1. 初始化奖励函数
2. 计算专家轨迹的特征期望
3. 使用当前奖励函数训练策略
4. 计算学习策略的特征期望
5. 更新奖励函数，使专家的特征期望与学习策略的特征期望差距最小
6. 重复步骤3-5，直到收敛
7. 使用最终学习到的奖励函数训练策略

## 输出结果

程序输出包括：
- 训练过程中的奖励、位置误差和模仿奖励曲线
- 最终训练好的奖励网络模型
- 最终训练好的策略模型
- 评估结果，包括位置误差和模仿奖励 